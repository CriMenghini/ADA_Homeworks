{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 04 - Applied ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Remarks for the easy reading of the work*:\n",
    "The data in use are stored in the folder `Data`, the description is available [here](https://github.com/ADAEPFL/Homework/blob/master/04%20-%20Applied%20ML/DATA.md).\n",
    "All the functions that are mentioned are stored in separate libraries that are specified at each step. For some of them the reading of the documentation is required to understand how certain results are obtained. \n",
    "The *Notebook* organisation is specified in the *Table of contents*.\n",
    "\n",
    "__Important__: due to the precence of interactive plot, we suggest you to visualize the notebook using the following [link](http://nbviewer.jupyter.org/github/CriMenghini/ADA_Homeworks/blob/master/Homework_4/Hw_4.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of contents\n",
    "1. [Predict the skin color of a soccer player](#task1)\n",
    "    1. [Exploratory Data Analysis, Feature Selection and Feature engineering](#EDA)\n",
    "     1. [Target variable](#target)\n",
    "     2. [Features](#features)\n",
    "    2. [Baseline model](#baseline)\n",
    "     1. [Preprocess variable to be used as input for the classifier](#preproc)\n",
    "     2. [Split train and test](#split)\n",
    "\t3. [Find the model](#tuning)\n",
    "     1. [Cross-validation for tuning parameters](#cvtuning)\n",
    "     2. [Cross-validation to assess the quality of the model](#cvmodel)\n",
    "    4. [Features importance](#impfeat)\n",
    "    5. [Balance the sample, create many regressors and then average the models](#balance)\n",
    "\t6. [*BONUS*](#bonus)\n",
    "2. [Cluster players with dark and light skin colors](#task2)\n",
    "    1. [Sub paragraph](#subparagraph1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. Predict the skin color of a soccer player <a name=\"task1\"></a>\n",
    "\n",
    "In this first task we train a *Random forest* classifier to be able to predict the skin color of a soccer player using the player description. In order to do so, we proceed pre-processing the data as first step then moving toward the choice of the model (interpret as the choice of parameters controlling the possible issues i.e. the *overfitting*). As required, we then switch to the inspection of the `feature_importances_` attribute and the discussion of the obtained results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import plotly\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns  \n",
    "from plots import *\n",
    "from balance_sample import *\n",
    "import plotly.tools as tls\n",
    "from sklearn import metrics\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt \n",
    "from data_preprocessing import *\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plotly.tools.set_credentials_file(username='crimenghini', api_key='t5q05yuxzu')\n",
    "plotly.tools.set_credentials_file(username='cristina.crocca', api_key='l1up4iv6pj')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>playerShort</th>\n",
       "      <th>player</th>\n",
       "      <th>club</th>\n",
       "      <th>leagueCountry</th>\n",
       "      <th>birthday</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>position</th>\n",
       "      <th>games</th>\n",
       "      <th>victories</th>\n",
       "      <th>...</th>\n",
       "      <th>rater2</th>\n",
       "      <th>refNum</th>\n",
       "      <th>refCountry</th>\n",
       "      <th>Alpha_3</th>\n",
       "      <th>meanIAT</th>\n",
       "      <th>nIAT</th>\n",
       "      <th>seIAT</th>\n",
       "      <th>meanExp</th>\n",
       "      <th>nExp</th>\n",
       "      <th>seExp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lucas-wilchez</td>\n",
       "      <td>Lucas Wilchez</td>\n",
       "      <td>Real Zaragoza</td>\n",
       "      <td>Spain</td>\n",
       "      <td>31.08.1983</td>\n",
       "      <td>177.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>Attacking Midfielder</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>GRC</td>\n",
       "      <td>0.326391</td>\n",
       "      <td>712.0</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>0.396000</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.002696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>john-utaka</td>\n",
       "      <td>John Utaka</td>\n",
       "      <td>Montpellier HSC</td>\n",
       "      <td>France</td>\n",
       "      <td>08.01.1982</td>\n",
       "      <td>179.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>Right Winger</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>ZMB</td>\n",
       "      <td>0.203375</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.010875</td>\n",
       "      <td>-0.204082</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.061504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abdon-prats</td>\n",
       "      <td>Abdón Prats</td>\n",
       "      <td>RCD Mallorca</td>\n",
       "      <td>Spain</td>\n",
       "      <td>17.12.1992</td>\n",
       "      <td>181.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>ESP</td>\n",
       "      <td>0.369894</td>\n",
       "      <td>1785.0</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.588297</td>\n",
       "      <td>1897.0</td>\n",
       "      <td>0.001002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pablo-mari</td>\n",
       "      <td>Pablo Marí</td>\n",
       "      <td>RCD Mallorca</td>\n",
       "      <td>Spain</td>\n",
       "      <td>31.08.1993</td>\n",
       "      <td>191.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>Center Back</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>ESP</td>\n",
       "      <td>0.369894</td>\n",
       "      <td>1785.0</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.588297</td>\n",
       "      <td>1897.0</td>\n",
       "      <td>0.001002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ruben-pena</td>\n",
       "      <td>Rubén Peña</td>\n",
       "      <td>Real Valladolid</td>\n",
       "      <td>Spain</td>\n",
       "      <td>18.07.1991</td>\n",
       "      <td>172.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>Right Midfielder</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>ESP</td>\n",
       "      <td>0.369894</td>\n",
       "      <td>1785.0</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.588297</td>\n",
       "      <td>1897.0</td>\n",
       "      <td>0.001002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     playerShort         player             club leagueCountry    birthday  \\\n",
       "0  lucas-wilchez  Lucas Wilchez    Real Zaragoza         Spain  31.08.1983   \n",
       "1     john-utaka     John Utaka  Montpellier HSC        France  08.01.1982   \n",
       "2    abdon-prats    Abdón Prats     RCD Mallorca         Spain  17.12.1992   \n",
       "3     pablo-mari     Pablo Marí     RCD Mallorca         Spain  31.08.1993   \n",
       "4     ruben-pena     Rubén Peña  Real Valladolid         Spain  18.07.1991   \n",
       "\n",
       "   height  weight              position  games  victories    ...     rater2  \\\n",
       "0   177.0    72.0  Attacking Midfielder      1          0    ...       0.50   \n",
       "1   179.0    82.0          Right Winger      1          0    ...       0.75   \n",
       "2   181.0    79.0                   NaN      1          0    ...        NaN   \n",
       "3   191.0    87.0           Center Back      1          1    ...        NaN   \n",
       "4   172.0    70.0      Right Midfielder      1          1    ...        NaN   \n",
       "\n",
       "   refNum  refCountry  Alpha_3   meanIAT    nIAT     seIAT   meanExp    nExp  \\\n",
       "0       1           1      GRC  0.326391   712.0  0.000564  0.396000   750.0   \n",
       "1       2           2      ZMB  0.203375    40.0  0.010875 -0.204082    49.0   \n",
       "2       3           3      ESP  0.369894  1785.0  0.000229  0.588297  1897.0   \n",
       "3       3           3      ESP  0.369894  1785.0  0.000229  0.588297  1897.0   \n",
       "4       3           3      ESP  0.369894  1785.0  0.000229  0.588297  1897.0   \n",
       "\n",
       "      seExp  \n",
       "0  0.002696  \n",
       "1  0.061504  \n",
       "2  0.001002  \n",
       "3  0.001002  \n",
       "4  0.001002  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data \n",
    "data = pd.read_csv('Data/CrowdstormingDataJuly1st.csv', sep = ',')\n",
    "# Take a look at the data head\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Exploratory Data Analysis, Feature Selection and Feature engineering <a name=\"EDA\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### a. Target variable <a name=\"target\"></a>\n",
    "\n",
    "Before proceeding with the exploration of the features, we focuse our attention on the target variable (`rater1`, `rater2`). In this case we face the folliwing issues:\n",
    "1. [*Absence of labels*](#absence): Not all the players have an `IDphoto`, thus the *raters* can not label the skin color. It results in a bunch of player not labeled. Since in this first task we work using the *Supervised* learning we drop out all the *dyads* that correspond to players whose picture is not available.\n",
    "\n",
    "2. [*Inconsistency of labels*](#inconsistency): The labels assigned by the two raters for some players disagree. In order to control this inconsistency we think about different approaches. \n",
    "    - Compute the mean of the assigned scores. Whether the classification problem is set up as a *multiclassification* problem (five classes according to the `data description` - 0, 0.25, 0.5, 0.75, 1), if the disagreement of the two classes is greater than 0.25 (absolute value) the computation of the average implies the creation of new classes. Otherwise, whether the classification problem is simplified to the *binary* classification (all those players that have been labeled with $0 < values \\leq 0.5$ belong to class 0, all those whose $0.5 < values \\leq 1$) the values obtained computing the average can be easily assigned to one of the two classes.\n",
    "    - Use the two scores vectors to train the model, defining a *multi target* model whether the problem is set up both as *multiclass* or *binary*.\n",
    "        \n",
    "3. [*Unbalanced sample*](#unbalance): the sample that we analyse turns to be *unbalanced*. It means that there are classes that are more present in the population. This verification leads to the necessity of using different metrics, rather the *simple* accuracy, to evaluate the model, and can be in some way faced using some tecniques to *rebalance* the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Absence of labels  <a name=\"absence\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop out the unlabeled players\n",
    "data_clean = data[(data.photoID.notnull())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are left with the data for the players that have a picture. We want to check whether given the picture both of the raters assuigned the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rater 1 does not label 0 players\n",
      "Rater 2 does not label 0 players\n"
     ]
    }
   ],
   "source": [
    "# How many players the rater 1 don't label?\n",
    "miss_rater_1 = sum(data_clean.rater1.isnull())\n",
    "# How many the rater 2?\n",
    "miss_rater_2 = sum(data_clean.rater2.isnull())\n",
    "\n",
    "print ('Rater 1 does not label', miss_rater_1, 'players')\n",
    "print ('Rater 2 does not label', miss_rater_2, 'players')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that both of them label all the players with a picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Inconsistency of labels <a name=\"inconsistency\"></a>\n",
    "\n",
    "##### 2.1. Handle `NaNs`\n",
    "Before analysing the *target* we should control whether there is the precence of `NaN` values, that can eventually lead to the elimitation of players, in the dataset, then we *aggregate* by the player. It is important to check the precence of null values before the aggregation for two reasons:\n",
    "* It is possible that some dyads do not contain certain values, it does not imply that in the dataset we can not find other dyads that contain the information. Hence, we remove the dyads or, if possible, assign the value (according to the kind of attribute) so that we don't loose the player.\n",
    "* The precence of `NaN` can cause problems whether an aggregation function is applied. That's because they may propagate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize the dictionary {key:value} whose key is the attributed with NaNs and value are the indices\n",
    "variables_with_nan = {}\n",
    "\n",
    "# For each attribute\n",
    "for attribute in data_clean.columns:\n",
    "    # Check if there are nans\n",
    "    index_nan = data_clean[attribute].isnull()\n",
    "    presence_nan = sum(index_nan)\n",
    "    \n",
    "    if presence_nan != 0:\n",
    "        variables_with_nan[attribute] = index_nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the variables with `NaNs` is listed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['height', 'seExp', 'nIAT', 'seIAT', 'meanExp', 'nExp', 'meanIAT', 'Alpha_3', 'weight', 'position'])\n"
     ]
    }
   ],
   "source": [
    "print(variables_with_nan.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed considering the attributes related to the players: `weight`, `height`, `position`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "players_attributes = ['weight', 'height', 'position']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we clean the dataframe, the documentation related to the function [`remove_nans`](data_preprocessing.py) provides the explanation related to the procedure used to remove `NaNs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_clean = remove_nans(data_clean, variables_with_nan, players_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove ~21% of the dyads, but the number of drop player is controlled by the approach used to remove the `NaNs`. In fact we remove just those player whose important description feature are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of removed dyads:  30425\n",
      "Percentage of the removed dyads:  20.84 %\n"
     ]
    }
   ],
   "source": [
    "print ('Number of removed dyads: ', data.shape[0] - data_clean.shape[0])\n",
    "print ('Percentage of the removed dyads: ', round((data.shape[0] - data_clean.shape[0])/len(data)*100,2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we aggregate by player and we observe that we proceed the analysis taking into account ~90% of the players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Group by the player\n",
    "player_data = data_clean.groupby('playerShort')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of players:  1419\n",
      "Percentage of analysed players:  89.47 %\n"
     ]
    }
   ],
   "source": [
    "print ('Number of players: ', len(player_data))\n",
    "print ('Percentage of analysed players: ', round(len(player_data)/1586*100, 2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using the aggregation functions for some attributes we check whether aggregating we risk to loose some information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each player belongs to:  1 club.\n",
      "Each player registers: 1 position.\n",
      "Each player registers:  1.0 weight.\n",
      "Each player registers:  1.0 height.\n"
     ]
    }
   ],
   "source": [
    "# Check that each player belongs to one club\n",
    "print ('Each player belongs to: ', player_data.agg({'club' : lambda x: len(set(x))})['club'].unique()[0], 'club.')\n",
    "# Check that each player registers one position\n",
    "print ('Each player registers:', player_data.agg({'position' : lambda x: len(set(x))})['position'].unique()[0], 'position.')\n",
    "# Check that each player registers one weight\n",
    "print ('Each player registers: ', player_data.agg({'weight' : lambda x: len(set(x))})['weight'].unique()[0], 'weight.')\n",
    "# Check that each player registers one height\n",
    "print ('Each player registers: ', player_data.agg({'height' : lambda x: len(set(x))})['height'].unique()[0], 'height.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the aggregation function\n",
    "players = player_data.agg({'club' : 'first',\n",
    "                           'leagueCountry' : 'first',\n",
    "                           'birthday' : 'first',\n",
    "                           'height' : 'first',\n",
    "                           'weight' : 'first',\n",
    "                           'position' : 'first',\n",
    "                           'games' : 'sum',\n",
    "                           'victories' : 'sum',\n",
    "                           'ties' : 'sum',\n",
    "                           'defeats' : 'sum',\n",
    "                           'goals' : 'sum',\n",
    "                           'yellowCards': 'sum',\n",
    "                           'yellowReds': 'sum',\n",
    "                           'redCards' : 'sum',\n",
    "                           'rater1' : 'mean',\n",
    "                           'rater2' : 'mean',\n",
    "                           #'refNum' : 'count',\n",
    "                           #'refCountry' : 'count',\n",
    "                           #'meanIAT' : 'mean',\n",
    "                           #'meanExp' : 'mean'\n",
    "                          })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>height</th>\n",
       "      <th>redCards</th>\n",
       "      <th>games</th>\n",
       "      <th>yellowReds</th>\n",
       "      <th>weight</th>\n",
       "      <th>position</th>\n",
       "      <th>rater1</th>\n",
       "      <th>ties</th>\n",
       "      <th>rater2</th>\n",
       "      <th>club</th>\n",
       "      <th>goals</th>\n",
       "      <th>leagueCountry</th>\n",
       "      <th>yellowCards</th>\n",
       "      <th>defeats</th>\n",
       "      <th>victories</th>\n",
       "      <th>birthday</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>playerShort</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aaron-hughes</th>\n",
       "      <td>182.0</td>\n",
       "      <td>0</td>\n",
       "      <td>654</td>\n",
       "      <td>0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>Center Back</td>\n",
       "      <td>0.25</td>\n",
       "      <td>179</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Fulham FC</td>\n",
       "      <td>9</td>\n",
       "      <td>England</td>\n",
       "      <td>19</td>\n",
       "      <td>228</td>\n",
       "      <td>247</td>\n",
       "      <td>08.11.1979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaron-hunt</th>\n",
       "      <td>183.0</td>\n",
       "      <td>1</td>\n",
       "      <td>336</td>\n",
       "      <td>0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>Attacking Midfielder</td>\n",
       "      <td>0.00</td>\n",
       "      <td>73</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Werder Bremen</td>\n",
       "      <td>62</td>\n",
       "      <td>Germany</td>\n",
       "      <td>42</td>\n",
       "      <td>122</td>\n",
       "      <td>141</td>\n",
       "      <td>04.09.1986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaron-lennon</th>\n",
       "      <td>165.0</td>\n",
       "      <td>0</td>\n",
       "      <td>412</td>\n",
       "      <td>0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>Right Midfielder</td>\n",
       "      <td>0.25</td>\n",
       "      <td>97</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Tottenham Hotspur</td>\n",
       "      <td>31</td>\n",
       "      <td>England</td>\n",
       "      <td>11</td>\n",
       "      <td>115</td>\n",
       "      <td>200</td>\n",
       "      <td>16.04.1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaron-ramsey</th>\n",
       "      <td>178.0</td>\n",
       "      <td>1</td>\n",
       "      <td>260</td>\n",
       "      <td>0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>Center Midfielder</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Arsenal FC</td>\n",
       "      <td>39</td>\n",
       "      <td>England</td>\n",
       "      <td>31</td>\n",
       "      <td>68</td>\n",
       "      <td>150</td>\n",
       "      <td>26.12.1990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abdelhamid-el-kaoutari</th>\n",
       "      <td>180.0</td>\n",
       "      <td>2</td>\n",
       "      <td>124</td>\n",
       "      <td>4</td>\n",
       "      <td>73.0</td>\n",
       "      <td>Center Back</td>\n",
       "      <td>0.25</td>\n",
       "      <td>40</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Montpellier HSC</td>\n",
       "      <td>1</td>\n",
       "      <td>France</td>\n",
       "      <td>8</td>\n",
       "      <td>43</td>\n",
       "      <td>41</td>\n",
       "      <td>17.03.1990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        height  redCards  games  yellowReds  weight  \\\n",
       "playerShort                                                           \n",
       "aaron-hughes             182.0         0    654           0    71.0   \n",
       "aaron-hunt               183.0         1    336           0    73.0   \n",
       "aaron-lennon             165.0         0    412           0    63.0   \n",
       "aaron-ramsey             178.0         1    260           0    76.0   \n",
       "abdelhamid-el-kaoutari   180.0         2    124           4    73.0   \n",
       "\n",
       "                                    position  rater1  ties  rater2  \\\n",
       "playerShort                                                          \n",
       "aaron-hughes                     Center Back    0.25   179    0.00   \n",
       "aaron-hunt              Attacking Midfielder    0.00    73    0.25   \n",
       "aaron-lennon                Right Midfielder    0.25    97    0.25   \n",
       "aaron-ramsey               Center Midfielder    0.00    42    0.00   \n",
       "abdelhamid-el-kaoutari           Center Back    0.25    40    0.25   \n",
       "\n",
       "                                     club  goals leagueCountry  yellowCards  \\\n",
       "playerShort                                                                   \n",
       "aaron-hughes                    Fulham FC      9       England           19   \n",
       "aaron-hunt                  Werder Bremen     62       Germany           42   \n",
       "aaron-lennon            Tottenham Hotspur     31       England           11   \n",
       "aaron-ramsey                   Arsenal FC     39       England           31   \n",
       "abdelhamid-el-kaoutari    Montpellier HSC      1        France            8   \n",
       "\n",
       "                        defeats  victories    birthday  \n",
       "playerShort                                             \n",
       "aaron-hughes                228        247  08.11.1979  \n",
       "aaron-hunt                  122        141  04.09.1986  \n",
       "aaron-lennon                115        200  16.04.1987  \n",
       "aaron-ramsey                 68        150  26.12.1990  \n",
       "abdelhamid-el-kaoutari       43         41  17.03.1990  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "players.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2. Analyse the target  <a name=\"rater12\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the new dataframe we extract the two variables that correspond to the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract labels\n",
    "label_1 = players['rater1']\n",
    "label_2 = players['rater2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the distribution of the labels related to the two raters are different. It shows the disagreement aforementioned. In particular, the first classifies the 75% of the players as *very light skin*, *light skin*, the number of players classified as *dark skin* or *very dark skin* is so low that the are outside the *Inter-Quartile Range*. The second rated evidence the tendency of giving higher scores. Since we do not have another rater to compare, we can't make an assumption on the reliability of the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~cristina.crocca/27.embed\" height=\"525\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#boxplot_raters(label_1, label_2)\n",
    "tls.embed('https://plot.ly/~cristina.crocca/27')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Unbalanced sample <a name=\"unbalance\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the view of what we observed in the [previous](#rater12) section, we see that our sample suffers of the lack of samples that are recognized as *dark skin* or *very dark skin*. We show more clearly with the following plot the unbalancement. In particular we see that both the *Rater 1* and *Rater 2* classify in the first three classes more than the 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stacked_plot(label_1,label_2)\n",
    "#tls.embed(\"https://plot.ly/~cristina.crocca/24\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This verification implies important consideration for the further analysis. \n",
    "1. We redifine the goal of our analysis as a *Binary Classification* problem. We consider it enough to distinguish a \"light skin\" player from a \"dark skin\". In particular the Labels are encoded according to the intervals defined [above](#target). This choice can help us in facing the *unbalancement* of our sample and reduce the complexity of our classification problem. \n",
    "2. So far we used the *Accuracy* metric in order to evaluate the accuracy of the model. We will replace it with some other metric. The reason behind it comes from the definition of the accuracy. It is a ratio of the correct prediction over the total sample to predict. Due to the presence of a class (*light skin*) that represents the majority in the sample, the classifier is able to classify this class well. In particular it tends to classify all the observation as that class (we will look at the confusion matrices later). Thus, the accuracy results to be very good just because of the high precence of the class in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we encode the labels according to the binary classification problem. Before encoding the labels we merge the rates given by the two raters by averaging the scores. The function is stored in [`data_preprocessing`](data_preprocessing.py) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_merged = (label_1 + label_2)/2\n",
    "labels = label_merged.apply(binary_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "players.drop('rater1', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "players.drop('rater2', axis= 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Features <a name=\"features\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we focuse on the study of the other features in our dataset. In particular, for the moment we avoid to take into account the information related to the [referee](#referee) (features realated to the `IAT` and `Exp`, `refCountry`).\n",
    "In an ideal world, where the racism is not present on a soccer's field, the [phisical characteristics](#physical) of a player (and the features that derives from them) should be the only really useful to classify a player as black or white. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Physical features <a name=\"physical\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, the following plot tries to visualize how some features vary according to the the skin color of a player. We choose as features to plot:\n",
    "- `leagueCountry`: It may be possible that in some leagues there is an higher presece of players with different skin colors.\n",
    "- `club`: Some clubs may decide to buy players according to the physical characteristics of players with *light* or *dark* skin color.\n",
    "- `yellowCards`: We focuse on the average number of yellow cards registered in each team. \n",
    "- `weight`, `height`: Physical characteristics. It may be plausible that *light* skin soccer players have different pysical characteristics respect the *dark* skin ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each bubble represents a `club`. The diameter of the bubbles shows the proportion of *dark* skin players in the club. The colour of the bubbles corresponds to the `League` the club is part of. The legend shows different sizes, bigger is the point greater is the average percentage of *dark* players in the `League`. The pop-up of each bubbles describe the characteristics of the `club`. It is important to notice that there are few bubbles that result to be extramly big and that register the 100% presence of *dark* skin players, just because he is the only player in the team!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~cristina.crocca/0.embed\" height=\"525\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bubble_plot(labels, players, 'yellowCards', 'mean', 'weight', 'mean', 'height', 'mean')\n",
    "tls.embed(\"https://plot.ly/~cristina.crocca/0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot is interesting. We can infer that knowing the League which the player belongs may be a good variable for our model. Taking a look at the bubbles of the same color, we see that the clubs whose proportion of *dark* skin players is similar tend to have similar values related to the average height and weight. Respect the number of `Yellow cards`, using the pop-ups, we are not able to find a kind of *path* toward the skin color."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Referee features <a name=\"referee\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing that we inspect is the distribution of the average `meanIAT` and `meanExp`. We infer that the average `meanIAT` and `meanExp` of the referees who umpire the games of *light* skin players tend to have higher values. We do not think that the features related to this scores should be part of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~cristina.crocca/6.embed\" height=\"525\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#boxplot_plotly(players, labels, 'meanIAT')\n",
    "tls.embed('https://plot.ly/~cristina.crocca/6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~cristina.crocca/9.embed\" height=\"525\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#boxplot_plotly(players, labels, 'meanExp')\n",
    "tls.embed('https://plot.ly/~cristina.crocca/9')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check whether a relationship between the number of yellow cards obtained by a player (by making the distrinction between *light* and *dark* skin) appears to be, in some sense, dependent on the average of the `meanIAT` values of all the referee a player play with. As shown in the plot below, it is not possible to identify a real path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~cristina.crocca/13.embed\" height=\"525\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scatter_plot(players, labels)\n",
    "tls.embed('https://plot.ly/~cristina.crocca/13')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Baseline model  <a name=\"baseline\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model than we consider takes as inputs all the variables except the `refNum`, `refCountry`, the standard deviation of `meanIAT` and `meanExp` and the cardinality of the population the information (related to the latter features) have been inferred.\n",
    "\n",
    "Before feeding the model with the features, we proceed as follows:\n",
    "\n",
    "1. We [preprocess](#preproc) the different features:\n",
    " 1. Birthday: just keep the year\n",
    " 2. Categorization of numerical features when they present more than 12 different values\n",
    " 3. Encoding of the 'object' attributes according to what is required by the `sklearn`'s `RandomForest` classifier.\n",
    "\n",
    "2. We [split](#split) the entire data into train and test set (respectively 80% and 20%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess variable to be used as input for the classifier  <a name=\"preproc\"></a>\n",
    "\n",
    "The procedures used to preprocess the data are wrapped in functions sored in the [`data_preprocessing`](data_preprocessing.py) library. Inside the function is provided the documentation to understand what we do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Keep only the year of the birthday\n",
    "players['birthday'] = players['birthday'].apply(lambda x: float(x.split('.')[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object features:  ['position', 'club', 'leagueCountry']\n",
      "Numerical features:  ['height', 'games', 'weight', 'ties', 'goals', 'yellowCards', 'defeats', 'victories', 'birthday']\n"
     ]
    }
   ],
   "source": [
    "# Get the string variables\n",
    "object_features = [i for i in players.columns if players[i].dtypes == 'object']\n",
    "print ('Object features: ', object_features)\n",
    "\n",
    "\n",
    "# Get the list of the numerica variables\n",
    "numerical_features = [i for i in players.columns if (players[i].dtypes == 'int64' or players[i].dtypes == 'float64') and len(players[i].unique()) > 12]\n",
    "print ('Numerical features: ',numerical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encode string variables\n",
    "for feature in object_features:\n",
    "    encode_string_variable(players, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Categorise the numerical features\n",
    "for i in range(len(numerical_features)):\n",
    "    players[numerical_features[i]] = players[numerical_features[i]].apply(partial(categorisation, create_bins(players, numerical_features[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the pre-processed dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>height</th>\n",
       "      <th>redCards</th>\n",
       "      <th>games</th>\n",
       "      <th>yellowReds</th>\n",
       "      <th>weight</th>\n",
       "      <th>position</th>\n",
       "      <th>ties</th>\n",
       "      <th>club</th>\n",
       "      <th>goals</th>\n",
       "      <th>leagueCountry</th>\n",
       "      <th>yellowCards</th>\n",
       "      <th>defeats</th>\n",
       "      <th>victories</th>\n",
       "      <th>birthday</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>playerShort</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>arrizabalaga</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cristian-alvarez_3</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marc-ziegler</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predrag-stevanovic</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jose-nunes</th>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    height  redCards  games  yellowReds  weight  position  \\\n",
       "playerShort                                                                 \n",
       "arrizabalaga            16         0      0           0       8         5   \n",
       "cristian-alvarez_3      16         0      2           0      10         5   \n",
       "marc-ziegler            21         0      3           0       9         5   \n",
       "predrag-stevanovic      10         0      1           0       2         0   \n",
       "jose-nunes              15         4      4           1       8         1   \n",
       "\n",
       "                    ties  club  goals  leagueCountry  yellowCards  defeats  \\\n",
       "playerShort                                                                  \n",
       "arrizabalaga           0     9      0              3            0        0   \n",
       "cristian-alvarez_3     3    25      0              3            1        3   \n",
       "marc-ziegler           3    84      0              2            0        3   \n",
       "predrag-stevanovic     1    86      0              2            0        1   \n",
       "jose-nunes             5    58      0              3            4        5   \n",
       "\n",
       "                    victories  birthday  \n",
       "playerShort                              \n",
       "arrizabalaga                0        13  \n",
       "cristian-alvarez_3          1         8  \n",
       "marc-ziegler                2         2  \n",
       "predrag-stevanovic          0        11  \n",
       "jose-nunes                  3         3  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "players.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Split train and test  <a name=\"split\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(players, labels, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the fact that our sample is [unbalanced](#unbalance), we try to mitigate its effect exploiting some `sklearn` [spells](http://gph.is/1sGDBKR). In particular, we provide to the model the weigths of the two classes inside the train population, in such a way that it uses it during the training. The code is provided in the [`balance_sample`](balance_sample.py) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute the sample weights of the train\n",
    "sample_weights = weight_sample(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we setu up our classifier. For the *baseline* model we do not care about the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=100, random_state=1, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we train the classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_forest = forest.fit(X_train, y_train, sample_weight= sample_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And get the predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict = train_forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first we get the *accuracy* of the model.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model:  0.841549295775\n"
     ]
    }
   ],
   "source": [
    "print ('Accuracy of the model: ', metrics.accuracy_score(y_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting no efforts the classifere already returns a good level of accuracy, that is the proportion of *True positive* and *True false* over the all population to predict. To be sure that the accuracy is a good metric to consider we take a look at the confusion matrix <a name=\"confmatr\"></a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      " [[237   2]\n",
      " [ 43   2]]\n"
     ]
    }
   ],
   "source": [
    "print ('Confusion matrix: \\n',\n",
    "       metrics.confusion_matrix(y_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see the classifier is able to classify correctly almost all the players with *Light* skin, but it commits a lot of mistakes in the classification of the *Dark* skin players. Infact only a little portion is correctly classified  (recall score considering the *Dark skin* as the positive value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall for the Dark skin class:  0.0444444444444\n"
     ]
    }
   ],
   "source": [
    "print ('Recall for the Dark skin class: ', metrics.recall_score(y_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This verification suggests us that, due to the unbalance of the sample, the accuracy is not the most appripriate way to measure the quality of our model. So we use as metrics the ROC-AUC <a name=\"roc_auc\"></a> score and the [F_BETA](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html) that is the weighted harmonic mean of precision and recall. In particular we assign to *beta* a value greater than 1 to give more relevance to the *recall*, in order to check whether our classifier is aslso able to recognize the *dark* skin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC :  0.518038121804\n"
     ]
    }
   ],
   "source": [
    "print ('AUC : ',metrics.roc_auc_score(y_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of the AUC shows that our classifier is not so far from a random classifier!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F beta :  0.0553920096125\n"
     ]
    }
   ],
   "source": [
    "print ('F beta : ', metrics.fbeta_score(y_test, predict, beta=1.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metric above gets value between 0 and 1. Its optimal value is 1 and its worst value is 0. We can't definitely say the our estimator is good enough!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### C. Find the model <a name=\"tuning\"></a>\n",
    "\n",
    "Now we proceed with the cross-validation to do two things:\n",
    "1. Tuning the parameters *max_depth* and *n_estimators*, namely we want to control how the model behaves when we reduce or increase its complexity.\n",
    "2. We run a simple cross-validation to check the quality of the classifier with different data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Cross-validation for tuning parameters <a name=\"cvtuning\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The run CV is a *K-folders* where K=10. The function used is the [`tuning_cv`](balance_sample.py), the documentation inside the library provides all the information about the implemented procedure. It recalls the [`cross_validation`](balance_sample.py) that we implemented from scratch in order to get additional information like the train \"accuracy\" and the standard deviation of both the *train* and *test* \"accuracy\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "average_roc_train, std_roc_train, average_fbeta_train, std_fbeta_train, average_roc_test, std_roc_test, average_fbeta_test, std_fbeta_test, couples_estimators = tuning_cv(players, labels, list_depths = range(4,50, 5), list_numbers_estimators = range(4,100, 5)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, for each combination of `depth` and `number of estimators` we plot the average AUC and Fbeta obtained during the *CV*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~cristina.crocca/19.embed\" height=\"525\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_plot(couples_estimators, average_roc_train, average_roc_test, average_fbeta_train, average_fbeta_test, plot_name='train_test')\n",
    "tls.embed('https://plot.ly/~cristina.crocca/19')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the graph we clearly see that our classifier overfits. The train error is always a way higher than the test error. Our model is too complex. We can notice a decreasing trend for the test *AUC* and *Fbeta* that shows that increasing the depth and the number of estimators of the classifiers (increasing the complexity of the model) we continue to reduce the bias in the model making it too sticky to the train data points and inducing a lack of generality. Moreovere, looking at the AUC scores, again, our model doesn't seem to be way better than a random classifier!!!\n",
    "\n",
    "Anyway, we have to choose the parameters that we want to use for our output model. We prefer those whose FBeta is higher. But before making the final choice we also take a look at the standard deviations of test FBeta. In particular, for the sake of the graph interpretation, we get focused on the first 30 combinations of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~cristina.crocca/23.embed\" height=\"525\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_bars(couples_estimators[:30], average_fbeta_test[:30], std_fbeta_test[:30])\n",
    "tls.embed('https://plot.ly/~cristina.crocca/23')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the parameters that we choose are `n_estimators`=9 and `max_depth`=24 (corresponding to the 14th combination). From the plot above we can see that, even if the variance of the *Fbeta* obtained during the *CV* is higher than the other group of *parameters couple* (from 4 to 9), the possible values of the *FBeta* are still better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-validation to assess the quality of the model <a name=\"cvmodel\"></a>\n",
    "\n",
    "Then, we run the *CV* to see the scores obtained by our model. In particular, we fit the classifier on the train and test sets used [above](#split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the classifier\n",
    "forest = RandomForestClassifier(n_estimators=9, max_depth=24, random_state=1, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train_forest = forest.fit(X_train, y_train, sample_weight= sample_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "predict = train_forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our results are worse than the previous ones in term of *accuracy*. It does not mean though, that the classifier is worse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model:  0.827464788732\n"
     ]
    }
   ],
   "source": [
    "print ('Accuracy of the model: ', metrics.accuracy_score(y_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, if we take a look at the confusion matrix we see that the number of true positive (*Dark* skin) increases (with respect to the [previous one](#confmatr). It means that our model now is a bit more capable of classifing not only one class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      " [[229  10]\n",
      " [ 39   6]]\n"
     ]
    }
   ],
   "source": [
    "print ('Confusion matrix: \\n',\n",
    "       metrics.confusion_matrix(y_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, unfortunately, the results do not show a significant improvement. Namely, if we look at the scores below they are better than the [previous](#roc_auc) but still very close to those of a random classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC :  0.545746164575\n"
     ]
    }
   ],
   "source": [
    "print ('AUC : ', metrics.roc_auc_score(y_test, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F beta :  0.155001400953\n"
     ]
    }
   ],
   "source": [
    "print ('F beta : ', metrics.fbeta_score(y_test, predict, beta=1.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we run the *CV* on 10 folds and we use as metric the F score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_fold = 10\n",
    "cv_scores = cross_val_score(forest, X_train, y_train, cv=n_fold, scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~cristina.crocca/31.embed\" height=\"525\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_cv_scores(n_fold, cv_scores)\n",
    "tls.embed('https://plot.ly/~cristina.crocca/31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On average, the F score in the 10-folds is : 0.0909843779409 . The model has still too low bias.\n"
     ]
    }
   ],
   "source": [
    "print ('On average, the F score in the 10-folds is :', np.mean(cv_scores), '. The model has still too low bias.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features importance <a name=\"impfeat\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now move to the evaluation of the features in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the features importance\n",
    "importances = forest.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_features_importance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-e9d0baf208c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_features_importance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimportances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://plot.ly/~cristina.crocca/21'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_features_importance' is not defined"
     ]
    }
   ],
   "source": [
    "plot_features_importance(players, importances)\n",
    "tls.embed('https://plot.ly/~cristina.crocca/21')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can happily announce that our beloved `pandas` wins its battle against racism! In fact, in our model, the features related to the behaviour of the referees are not as important as the ones that refer to the *\"description\"* of a player. So, whether we would be interested in performing the model with few input features, we could do it using those related only to the players."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Balance the sample, create many regressors and then average the models  <a name=\"balance\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the high effects that the composition of the sample has on the performances of our model, we decide to proceed as follow (the procedure is applied on the train, the test we will use later to perform the model).\n",
    "\n",
    "1. We observe that the weights of the train set are ~84% for the *light skin* and ~16% for the *dark skin*\n",
    "2. We create M new dataframe that will have the two classes in an equal proportion. \n",
    "3. We run a Random Forest to each dataset, then we compute the prediction.\n",
    "4. Average the M models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Classes precenc in the train data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Create the  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_class_0 = X_train[y_train == 0]\n",
    "X_class_1 = X_train[y_train == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def users_chunks(n, users_list):\n",
    "    \"\"\"This function returns the list of chunks that will be used during the multi-threading.\n",
    "    - n is the number of threads;\n",
    "    - user_list is the list of user to split.\"\"\"\n",
    "    num = float(len(users_list))/n \n",
    "    us_lists = [ users_list [i:i + int(num)] for i in range(0, (n-1)*int(num), int(num))]\n",
    "    us_lists.append(users_list[(n-1)*int(num):])\n",
    "    return us_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weight_class = y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indexes = users_chunks(int(weight_class[0]/weight_class[1]),np.random.permutation(X_class_0.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_1 = pd.concat([X_class_0.iloc[indexes[0]], X_class_1], axis = 0)\n",
    "y_1_train = y_train[X_1.index]\n",
    "X_2 = pd.concat([X_class_0.iloc[indexes[1]], X_class_1], axis = 0)\n",
    "y_2_train = y_train[X_2.index]\n",
    "X_3 = pd.concat([X_class_0.iloc[indexes[2]], X_class_1], axis = 0)\n",
    "y_3_train = y_train[X_3.index]\n",
    "X_4 = pd.concat([X_class_0.iloc[indexes[3]], X_class_1], axis = 0)\n",
    "y_4_train = y_train[X_4.index]\n",
    "X_5 = pd.concat([X_class_0.iloc[indexes[4]], X_class_1], axis = 0)\n",
    "y_5_train = y_train[X_5.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=100,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-146-783994f5abbf>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-146-783994f5abbf>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ['accuracy', 'adjusted_rand_score', 'average_precision', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_median_absolute_error', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc'\u001b[0m\n\u001b[0m                                                                                                                                                                                                                                                                                                                                                                                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    " ['accuracy', 'adjusted_rand_score', 'average_precision', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_median_absolute_error', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64608259468940576"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(forest, X_1, y_1_train, cv=10, scoring='f1').mean()#(lasso, X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.51428571,  0.61538462,  0.64864865,  0.54545455,  0.62857143,\n",
       "        0.61904762,  0.34482759,  0.66666667,  0.64864865,  0.76923077])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(forest, X_2, y_2_train, cv=10, scoring='f1')#(lasso, X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.61538462,  0.63157895,  0.62857143,  0.54545455,  0.71794872,\n",
       "        0.72222222,  0.46666667,  0.75      ,  0.68421053,  0.7       ])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(forest, X_3, y_3_train, cv=10, scoring='f1')#(lasso, X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.71794872,  0.57142857,  0.64864865,  0.5       ,  0.7027027 ,\n",
       "        0.63414634,  0.20689655,  0.62857143,  0.58823529,  0.7027027 ])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(forest, X_4, y_4_train, cv=10, scoring='f1')#(lasso, X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_numbers_estimators = range(1,150, 5)\n",
    "list_depths = range(1,50, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n",
      "(1, 3)\n",
      "(1, 5)\n",
      "(1, 7)\n",
      "(1, 9)\n",
      "(1, 11)\n",
      "(1, 13)\n",
      "(1, 15)\n",
      "(1, 17)\n",
      "(1, 19)\n",
      "(1, 21)\n",
      "(1, 23)\n",
      "(1, 25)\n",
      "(1, 27)\n",
      "(1, 29)\n",
      "(1, 31)\n",
      "(1, 33)\n",
      "(1, 35)\n",
      "(1, 37)\n",
      "(1, 39)\n",
      "(1, 41)\n",
      "(1, 43)\n",
      "(1, 45)\n",
      "(1, 47)\n",
      "(1, 49)\n",
      "(6, 1)\n",
      "(6, 3)\n",
      "(6, 5)\n",
      "(6, 7)\n",
      "(6, 9)\n",
      "(6, 11)\n",
      "(6, 13)\n",
      "(6, 15)\n",
      "(6, 17)\n",
      "(6, 19)\n",
      "(6, 21)\n",
      "(6, 23)\n",
      "(6, 25)\n",
      "(6, 27)\n",
      "(6, 29)\n",
      "(6, 31)\n",
      "(6, 33)\n",
      "(6, 35)\n",
      "(6, 37)\n",
      "(6, 39)\n",
      "(6, 41)\n",
      "(6, 43)\n",
      "(6, 45)\n",
      "(6, 47)\n",
      "(6, 49)\n",
      "(11, 1)\n",
      "(11, 3)\n",
      "(11, 5)\n",
      "(11, 7)\n",
      "(11, 9)\n",
      "(11, 11)\n",
      "(11, 13)\n",
      "(11, 15)\n",
      "(11, 17)\n",
      "(11, 19)\n",
      "(11, 21)\n",
      "(11, 23)\n",
      "(11, 25)\n",
      "(11, 27)\n",
      "(11, 29)\n",
      "(11, 31)\n",
      "(11, 33)\n",
      "(11, 35)\n",
      "(11, 37)\n",
      "(11, 39)\n",
      "(11, 41)\n",
      "(11, 43)\n",
      "(11, 45)\n",
      "(11, 47)\n",
      "(11, 49)\n",
      "(16, 1)\n",
      "(16, 3)\n",
      "(16, 5)\n",
      "(16, 7)\n",
      "(16, 9)\n",
      "(16, 11)\n",
      "(16, 13)\n",
      "(16, 15)\n",
      "(16, 17)\n",
      "(16, 19)\n",
      "(16, 21)\n",
      "(16, 23)\n",
      "(16, 25)\n",
      "(16, 27)\n",
      "(16, 29)\n",
      "(16, 31)\n",
      "(16, 33)\n",
      "(16, 35)\n",
      "(16, 37)\n",
      "(16, 39)\n",
      "(16, 41)\n",
      "(16, 43)\n",
      "(16, 45)\n",
      "(16, 47)\n",
      "(16, 49)\n",
      "(21, 1)\n",
      "(21, 3)\n",
      "(21, 5)\n",
      "(21, 7)\n",
      "(21, 9)\n",
      "(21, 11)\n",
      "(21, 13)\n",
      "(21, 15)\n",
      "(21, 17)\n",
      "(21, 19)\n",
      "(21, 21)\n",
      "(21, 23)\n",
      "(21, 25)\n",
      "(21, 27)\n",
      "(21, 29)\n",
      "(21, 31)\n",
      "(21, 33)\n",
      "(21, 35)\n",
      "(21, 37)\n",
      "(21, 39)\n",
      "(21, 41)\n",
      "(21, 43)\n",
      "(21, 45)\n",
      "(21, 47)\n",
      "(21, 49)\n",
      "(26, 1)\n",
      "(26, 3)\n",
      "(26, 5)\n",
      "(26, 7)\n",
      "(26, 9)\n",
      "(26, 11)\n",
      "(26, 13)\n",
      "(26, 15)\n",
      "(26, 17)\n",
      "(26, 19)\n",
      "(26, 21)\n",
      "(26, 23)\n",
      "(26, 25)\n",
      "(26, 27)\n",
      "(26, 29)\n",
      "(26, 31)\n",
      "(26, 33)\n",
      "(26, 35)\n",
      "(26, 37)\n",
      "(26, 39)\n",
      "(26, 41)\n",
      "(26, 43)\n",
      "(26, 45)\n",
      "(26, 47)\n",
      "(26, 49)\n",
      "(31, 1)\n",
      "(31, 3)\n",
      "(31, 5)\n",
      "(31, 7)\n",
      "(31, 9)\n",
      "(31, 11)\n",
      "(31, 13)\n",
      "(31, 15)\n",
      "(31, 17)\n",
      "(31, 19)\n",
      "(31, 21)\n",
      "(31, 23)\n",
      "(31, 25)\n",
      "(31, 27)\n",
      "(31, 29)\n",
      "(31, 31)\n",
      "(31, 33)\n",
      "(31, 35)\n",
      "(31, 37)\n",
      "(31, 39)\n",
      "(31, 41)\n",
      "(31, 43)\n",
      "(31, 45)\n",
      "(31, 47)\n",
      "(31, 49)\n",
      "(36, 1)\n",
      "(36, 3)\n",
      "(36, 5)\n",
      "(36, 7)\n",
      "(36, 9)\n",
      "(36, 11)\n",
      "(36, 13)\n",
      "(36, 15)\n",
      "(36, 17)\n",
      "(36, 19)\n",
      "(36, 21)\n",
      "(36, 23)\n",
      "(36, 25)\n",
      "(36, 27)\n",
      "(36, 29)\n",
      "(36, 31)\n",
      "(36, 33)\n",
      "(36, 35)\n",
      "(36, 37)\n",
      "(36, 39)\n",
      "(36, 41)\n",
      "(36, 43)\n",
      "(36, 45)\n",
      "(36, 47)\n",
      "(36, 49)\n",
      "(41, 1)\n",
      "(41, 3)\n",
      "(41, 5)\n",
      "(41, 7)\n",
      "(41, 9)\n",
      "(41, 11)\n",
      "(41, 13)\n",
      "(41, 15)\n",
      "(41, 17)\n",
      "(41, 19)\n",
      "(41, 21)\n",
      "(41, 23)\n",
      "(41, 25)\n",
      "(41, 27)\n",
      "(41, 29)\n",
      "(41, 31)\n",
      "(41, 33)\n",
      "(41, 35)\n",
      "(41, 37)\n",
      "(41, 39)\n",
      "(41, 41)\n",
      "(41, 43)\n",
      "(41, 45)\n",
      "(41, 47)\n",
      "(41, 49)\n",
      "(46, 1)\n",
      "(46, 3)\n",
      "(46, 5)\n",
      "(46, 7)\n",
      "(46, 9)\n",
      "(46, 11)\n",
      "(46, 13)\n",
      "(46, 15)\n",
      "(46, 17)\n",
      "(46, 19)\n",
      "(46, 21)\n",
      "(46, 23)\n",
      "(46, 25)\n",
      "(46, 27)\n",
      "(46, 29)\n",
      "(46, 31)\n",
      "(46, 33)\n",
      "(46, 35)\n",
      "(46, 37)\n",
      "(46, 39)\n",
      "(46, 41)\n",
      "(46, 43)\n",
      "(46, 45)\n",
      "(46, 47)\n",
      "(46, 49)\n",
      "(51, 1)\n",
      "(51, 3)\n",
      "(51, 5)\n",
      "(51, 7)\n",
      "(51, 9)\n",
      "(51, 11)\n",
      "(51, 13)\n",
      "(51, 15)\n",
      "(51, 17)\n",
      "(51, 19)\n",
      "(51, 21)\n",
      "(51, 23)\n",
      "(51, 25)\n",
      "(51, 27)\n",
      "(51, 29)\n",
      "(51, 31)\n",
      "(51, 33)\n",
      "(51, 35)\n",
      "(51, 37)\n",
      "(51, 39)\n",
      "(51, 41)\n",
      "(51, 43)\n",
      "(51, 45)\n",
      "(51, 47)\n",
      "(51, 49)\n",
      "(56, 1)\n",
      "(56, 3)\n",
      "(56, 5)\n",
      "(56, 7)\n",
      "(56, 9)\n",
      "(56, 11)\n",
      "(56, 13)\n",
      "(56, 15)\n",
      "(56, 17)\n",
      "(56, 19)\n",
      "(56, 21)\n",
      "(56, 23)\n",
      "(56, 25)\n",
      "(56, 27)\n",
      "(56, 29)\n",
      "(56, 31)\n",
      "(56, 33)\n",
      "(56, 35)\n",
      "(56, 37)\n",
      "(56, 39)\n",
      "(56, 41)\n",
      "(56, 43)\n",
      "(56, 45)\n",
      "(56, 47)\n",
      "(56, 49)\n",
      "(61, 1)\n",
      "(61, 3)\n",
      "(61, 5)\n",
      "(61, 7)\n",
      "(61, 9)\n",
      "(61, 11)\n",
      "(61, 13)\n",
      "(61, 15)\n",
      "(61, 17)\n",
      "(61, 19)\n",
      "(61, 21)\n",
      "(61, 23)\n",
      "(61, 25)\n",
      "(61, 27)\n",
      "(61, 29)\n",
      "(61, 31)\n",
      "(61, 33)\n",
      "(61, 35)\n",
      "(61, 37)\n",
      "(61, 39)\n",
      "(61, 41)\n",
      "(61, 43)\n",
      "(61, 45)\n",
      "(61, 47)\n",
      "(61, 49)\n",
      "(66, 1)\n",
      "(66, 3)\n",
      "(66, 5)\n",
      "(66, 7)\n",
      "(66, 9)\n",
      "(66, 11)\n",
      "(66, 13)\n",
      "(66, 15)\n",
      "(66, 17)\n",
      "(66, 19)\n",
      "(66, 21)\n",
      "(66, 23)\n",
      "(66, 25)\n",
      "(66, 27)\n",
      "(66, 29)\n",
      "(66, 31)\n",
      "(66, 33)\n",
      "(66, 35)\n",
      "(66, 37)\n",
      "(66, 39)\n",
      "(66, 41)\n",
      "(66, 43)\n",
      "(66, 45)\n",
      "(66, 47)\n",
      "(66, 49)\n",
      "(71, 1)\n",
      "(71, 3)\n",
      "(71, 5)\n",
      "(71, 7)\n",
      "(71, 9)\n",
      "(71, 11)\n",
      "(71, 13)\n",
      "(71, 15)\n",
      "(71, 17)\n",
      "(71, 19)\n",
      "(71, 21)\n",
      "(71, 23)\n",
      "(71, 25)\n",
      "(71, 27)\n",
      "(71, 29)\n",
      "(71, 31)\n",
      "(71, 33)\n",
      "(71, 35)\n",
      "(71, 37)\n",
      "(71, 39)\n",
      "(71, 41)\n",
      "(71, 43)\n",
      "(71, 45)\n",
      "(71, 47)\n",
      "(71, 49)\n",
      "(76, 1)\n",
      "(76, 3)\n",
      "(76, 5)\n",
      "(76, 7)\n",
      "(76, 9)\n",
      "(76, 11)\n",
      "(76, 13)\n",
      "(76, 15)\n",
      "(76, 17)\n",
      "(76, 19)\n",
      "(76, 21)\n",
      "(76, 23)\n",
      "(76, 25)\n",
      "(76, 27)\n",
      "(76, 29)\n",
      "(76, 31)\n",
      "(76, 33)\n",
      "(76, 35)\n",
      "(76, 37)\n",
      "(76, 39)\n",
      "(76, 41)\n",
      "(76, 43)\n",
      "(76, 45)\n",
      "(76, 47)\n",
      "(76, 49)\n",
      "(81, 1)\n",
      "(81, 3)\n",
      "(81, 5)\n",
      "(81, 7)\n",
      "(81, 9)\n",
      "(81, 11)\n",
      "(81, 13)\n",
      "(81, 15)\n",
      "(81, 17)\n",
      "(81, 19)\n",
      "(81, 21)\n",
      "(81, 23)\n",
      "(81, 25)\n",
      "(81, 27)\n",
      "(81, 29)\n",
      "(81, 31)\n",
      "(81, 33)\n",
      "(81, 35)\n",
      "(81, 37)\n",
      "(81, 39)\n",
      "(81, 41)\n",
      "(81, 43)\n",
      "(81, 45)\n",
      "(81, 47)\n",
      "(81, 49)\n",
      "(86, 1)\n",
      "(86, 3)\n",
      "(86, 5)\n",
      "(86, 7)\n",
      "(86, 9)\n",
      "(86, 11)\n",
      "(86, 13)\n",
      "(86, 15)\n",
      "(86, 17)\n",
      "(86, 19)\n",
      "(86, 21)\n",
      "(86, 23)\n",
      "(86, 25)\n",
      "(86, 27)\n",
      "(86, 29)\n",
      "(86, 31)\n",
      "(86, 33)\n",
      "(86, 35)\n",
      "(86, 37)\n",
      "(86, 39)\n",
      "(86, 41)\n",
      "(86, 43)\n",
      "(86, 45)\n",
      "(86, 47)\n",
      "(86, 49)\n",
      "(91, 1)\n",
      "(91, 3)\n",
      "(91, 5)\n",
      "(91, 7)\n",
      "(91, 9)\n",
      "(91, 11)\n",
      "(91, 13)\n",
      "(91, 15)\n",
      "(91, 17)\n",
      "(91, 19)\n",
      "(91, 21)\n",
      "(91, 23)\n",
      "(91, 25)\n",
      "(91, 27)\n",
      "(91, 29)\n",
      "(91, 31)\n",
      "(91, 33)\n",
      "(91, 35)\n",
      "(91, 37)\n",
      "(91, 39)\n",
      "(91, 41)\n",
      "(91, 43)\n",
      "(91, 45)\n",
      "(91, 47)\n",
      "(91, 49)\n",
      "(96, 1)\n",
      "(96, 3)\n",
      "(96, 5)\n",
      "(96, 7)\n",
      "(96, 9)\n",
      "(96, 11)\n",
      "(96, 13)\n",
      "(96, 15)\n",
      "(96, 17)\n",
      "(96, 19)\n",
      "(96, 21)\n",
      "(96, 23)\n",
      "(96, 25)\n",
      "(96, 27)\n",
      "(96, 29)\n",
      "(96, 31)\n",
      "(96, 33)\n",
      "(96, 35)\n",
      "(96, 37)\n",
      "(96, 39)\n",
      "(96, 41)\n",
      "(96, 43)\n",
      "(96, 45)\n",
      "(96, 47)\n",
      "(96, 49)\n",
      "(101, 1)\n",
      "(101, 3)\n",
      "(101, 5)\n",
      "(101, 7)\n",
      "(101, 9)\n",
      "(101, 11)\n",
      "(101, 13)\n",
      "(101, 15)\n",
      "(101, 17)\n",
      "(101, 19)\n",
      "(101, 21)\n",
      "(101, 23)\n",
      "(101, 25)\n",
      "(101, 27)\n",
      "(101, 29)\n",
      "(101, 31)\n",
      "(101, 33)\n",
      "(101, 35)\n",
      "(101, 37)\n",
      "(101, 39)\n",
      "(101, 41)\n",
      "(101, 43)\n",
      "(101, 45)\n",
      "(101, 47)\n",
      "(101, 49)\n",
      "(106, 1)\n",
      "(106, 3)\n",
      "(106, 5)\n",
      "(106, 7)\n",
      "(106, 9)\n",
      "(106, 11)\n",
      "(106, 13)\n",
      "(106, 15)\n",
      "(106, 17)\n",
      "(106, 19)\n",
      "(106, 21)\n",
      "(106, 23)\n",
      "(106, 25)\n",
      "(106, 27)\n",
      "(106, 29)\n",
      "(106, 31)\n",
      "(106, 33)\n",
      "(106, 35)\n",
      "(106, 37)\n",
      "(106, 39)\n",
      "(106, 41)\n",
      "(106, 43)\n",
      "(106, 45)\n",
      "(106, 47)\n",
      "(106, 49)\n",
      "(111, 1)\n",
      "(111, 3)\n",
      "(111, 5)\n",
      "(111, 7)\n",
      "(111, 9)\n",
      "(111, 11)\n",
      "(111, 13)\n",
      "(111, 15)\n",
      "(111, 17)\n",
      "(111, 19)\n",
      "(111, 21)\n",
      "(111, 23)\n",
      "(111, 25)\n",
      "(111, 27)\n",
      "(111, 29)\n",
      "(111, 31)\n",
      "(111, 33)\n",
      "(111, 35)\n",
      "(111, 37)\n",
      "(111, 39)\n",
      "(111, 41)\n",
      "(111, 43)\n",
      "(111, 45)\n",
      "(111, 47)\n",
      "(111, 49)\n",
      "(116, 1)\n",
      "(116, 3)\n",
      "(116, 5)\n",
      "(116, 7)\n",
      "(116, 9)\n",
      "(116, 11)\n",
      "(116, 13)\n",
      "(116, 15)\n",
      "(116, 17)\n",
      "(116, 19)\n",
      "(116, 21)\n",
      "(116, 23)\n",
      "(116, 25)\n",
      "(116, 27)\n",
      "(116, 29)\n",
      "(116, 31)\n",
      "(116, 33)\n",
      "(116, 35)\n",
      "(116, 37)\n",
      "(116, 39)\n",
      "(116, 41)\n",
      "(116, 43)\n",
      "(116, 45)\n",
      "(116, 47)\n",
      "(116, 49)\n",
      "(121, 1)\n",
      "(121, 3)\n",
      "(121, 5)\n",
      "(121, 7)\n",
      "(121, 9)\n",
      "(121, 11)\n",
      "(121, 13)\n",
      "(121, 15)\n",
      "(121, 17)\n",
      "(121, 19)\n",
      "(121, 21)\n",
      "(121, 23)\n",
      "(121, 25)\n",
      "(121, 27)\n",
      "(121, 29)\n",
      "(121, 31)\n",
      "(121, 33)\n",
      "(121, 35)\n",
      "(121, 37)\n",
      "(121, 39)\n",
      "(121, 41)\n",
      "(121, 43)\n",
      "(121, 45)\n",
      "(121, 47)\n",
      "(121, 49)\n",
      "(126, 1)\n",
      "(126, 3)\n",
      "(126, 5)\n",
      "(126, 7)\n",
      "(126, 9)\n",
      "(126, 11)\n",
      "(126, 13)\n",
      "(126, 15)\n",
      "(126, 17)\n",
      "(126, 19)\n",
      "(126, 21)\n",
      "(126, 23)\n",
      "(126, 25)\n",
      "(126, 27)\n",
      "(126, 29)\n",
      "(126, 31)\n",
      "(126, 33)\n",
      "(126, 35)\n",
      "(126, 37)\n",
      "(126, 39)\n",
      "(126, 41)\n",
      "(126, 43)\n",
      "(126, 45)\n",
      "(126, 47)\n",
      "(126, 49)\n",
      "(131, 1)\n",
      "(131, 3)\n",
      "(131, 5)\n",
      "(131, 7)\n",
      "(131, 9)\n",
      "(131, 11)\n",
      "(131, 13)\n",
      "(131, 15)\n",
      "(131, 17)\n",
      "(131, 19)\n",
      "(131, 21)\n",
      "(131, 23)\n",
      "(131, 25)\n",
      "(131, 27)\n",
      "(131, 29)\n",
      "(131, 31)\n",
      "(131, 33)\n",
      "(131, 35)\n",
      "(131, 37)\n",
      "(131, 39)\n",
      "(131, 41)\n",
      "(131, 43)\n",
      "(131, 45)\n",
      "(131, 47)\n",
      "(131, 49)\n",
      "(136, 1)\n",
      "(136, 3)\n",
      "(136, 5)\n",
      "(136, 7)\n",
      "(136, 9)\n",
      "(136, 11)\n",
      "(136, 13)\n",
      "(136, 15)\n",
      "(136, 17)\n",
      "(136, 19)\n",
      "(136, 21)\n",
      "(136, 23)\n",
      "(136, 25)\n",
      "(136, 27)\n",
      "(136, 29)\n",
      "(136, 31)\n",
      "(136, 33)\n",
      "(136, 35)\n",
      "(136, 37)\n",
      "(136, 39)\n",
      "(136, 41)\n",
      "(136, 43)\n",
      "(136, 45)\n",
      "(136, 47)\n",
      "(136, 49)\n",
      "(141, 1)\n",
      "(141, 3)\n",
      "(141, 5)\n",
      "(141, 7)\n",
      "(141, 9)\n",
      "(141, 11)\n",
      "(141, 13)\n",
      "(141, 15)\n",
      "(141, 17)\n",
      "(141, 19)\n",
      "(141, 21)\n",
      "(141, 23)\n",
      "(141, 25)\n",
      "(141, 27)\n",
      "(141, 29)\n",
      "(141, 31)\n",
      "(141, 33)\n",
      "(141, 35)\n",
      "(141, 37)\n",
      "(141, 39)\n",
      "(141, 41)\n",
      "(141, 43)\n",
      "(141, 45)\n",
      "(141, 47)\n",
      "(141, 49)\n",
      "(146, 1)\n",
      "(146, 3)\n",
      "(146, 5)\n",
      "(146, 7)\n",
      "(146, 9)\n",
      "(146, 11)\n",
      "(146, 13)\n",
      "(146, 15)\n",
      "(146, 17)\n",
      "(146, 19)\n",
      "(146, 21)\n",
      "(146, 23)\n",
      "(146, 25)\n",
      "(146, 27)\n",
      "(146, 29)\n",
      "(146, 31)\n",
      "(146, 33)\n",
      "(146, 35)\n",
      "(146, 37)\n",
      "(146, 39)\n",
      "(146, 41)\n",
      "(146, 43)\n",
      "(146, 45)\n",
      "(146, 47)\n",
      "(146, 49)\n"
     ]
    }
   ],
   "source": [
    "average_accuracy_train = []\n",
    "std_accuracy_train = []\n",
    "average_precision_train = []\n",
    "std_precision_train = []\n",
    "average_f_score_train = []\n",
    "std_f_score_train = []\n",
    "average_recall_train = []\n",
    "std_recall_train = []\n",
    "average_roc_train = []\n",
    "std_roc_train = []\n",
    "\n",
    "\n",
    "average_accuracy_test = []\n",
    "std_accuracy_test = []\n",
    "average_precision_test = []\n",
    "std_precision_test = []\n",
    "average_f_score_test = []\n",
    "std_f_score_test = []\n",
    "average_recall_test = []\n",
    "std_recall_test = []\n",
    "average_roc_test = []\n",
    "std_roc_test = []\n",
    "\n",
    "couples_estimators = []\n",
    "\n",
    "for estimator in list_numbers_estimators:\n",
    "    for depth in list_depths:\n",
    "        couples_estimators += [(estimator, depth)]\n",
    "        print ((estimator, depth))\n",
    "        cv = cross_validation(X_1, y_1_train, estimator, depth)\n",
    "\n",
    "        average_accuracy_train += [np.mean(cv[0])]\n",
    "        std_accuracy_train += [np.std(cv[0])]\n",
    "        average_precision_train += [np.mean(cv[1])]\n",
    "        std_precision_train += [np.std(cv[1])]\n",
    "        average_f_score_train += [np.mean(cv[2])]\n",
    "        std_f_score_train += [np.std(cv[2])]\n",
    "        average_recall_train += [np.mean(cv[3])]\n",
    "        std_recall_train += [np.std(cv[3])]\n",
    "        average_roc_train += [np.mean(cv[4])]\n",
    "        std_roc_train += [np.std(cv[4])]\n",
    "        \n",
    "        \n",
    "        \n",
    "        average_accuracy_test += [np.mean(cv[5])]\n",
    "        std_accuracy_test  += [np.std(cv[5])]\n",
    "        average_precision_test += [np.mean(cv[6])]\n",
    "        std_precision_test += [np.std(cv[6])]\n",
    "        average_f_score_test += [np.mean(cv[7])]\n",
    "        std_f_score_test += [np.std(cv[7])]\n",
    "        average_recall_test += [np.mean(cv[8])]\n",
    "        std_recall_test += [np.std(cv[8])]\n",
    "        average_roc_test += [np.mean(cv[9])]\n",
    "        std_roc_test += [np.std(cv[9])]\n",
    "    \n",
    "        #print ('The average accuracy:', np.mean(cv[0]),'using as number of estimators ', estimator, 'and the maximum depth of the tree ', depth)\n",
    "        #print ('The average precision:', np.mean(cv[1]),' using as number of estimators:', estimator, 'and the maximum depth of the tree:', depth)\n",
    "        #print ('The average f_score:', np.mean(cv[2]),' using as number of estimators:', estimator, 'and the maximum depth of the tree:', depth)\n",
    "        #print ('The average recall:', np.mean(cv[3]),' using as number of estimators:', estimator, 'and the maximum depth of the tree:', depth)\n",
    "        #print ('*'*50)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=100,random_state=1)\n",
    "# Train classifier 1\n",
    "train_forest = forest.fit(X_1, y_1_train)\n",
    "predict_1 = train_forest.predict(X_test)\n",
    "# Train classifier 2\n",
    "train_forest = forest.fit(X_2, y_2_train)\n",
    "predict_2 = train_forest.predict(X_test)\n",
    "# Train classifier 3\n",
    "train_forest = forest.fit(X_3, y_3_train)\n",
    "predict_3 = train_forest.predict(X_test)\n",
    "# Train classifier 4\n",
    "train_forest = forest.fit(X_4, y_4_train)\n",
    "predict_4 = train_forest.predict(X_test)\n",
    "# Train classifier 5\n",
    "train_forest = forest.fit(X_5, y_5_train)\n",
    "predict_5 = train_forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction_df = pd.DataFrame([predict_1.T, predict_2.T, predict_3.T, predict_4.T, predict_5.T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "average_model_predictions = []\n",
    "for i in prediction_df.columns:\n",
    "    #print (i)\n",
    "    occurrences = prediction_df[i].value_counts()\n",
    "    if len(occurrences) == 1:\n",
    "        average_model_predictions += [occurrences.index[0]]\n",
    "    else:\n",
    "        if np.any(occurrences[occurrences.index == 1] > occurrences[occurrences.index == 0]):\n",
    "            average_model_predictions += [1]\n",
    "        else:\n",
    "            average_model_predictions += [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60557880055787994"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.roc_auc_score(y_test, average_model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23762376237623761"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.precision_score(y_test, average_model_predictions)#, average = 'weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[162,  77],\n",
       "       [ 21,  24]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test, average_model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53333333333333333"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.recall_score(y_test, average_model_predictions)#, average = 'weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32876712328767127"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_test, average_model_predictions)#, average = 'weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65492957746478875"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test, average_model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41996583791990888"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.fbeta_score(y_test, average_model_predictions, beta=1.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### *BONUS* <a name=\"bonus\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_learning_curve(RandomForestClassifier(n_estimators=75, max_depth=75), 'ciao', X_train, y_train, cv = ShuffleSplit(n_splits=20, test_size=0.2, random_state=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_learning_curve(RandomForestClassifier(n_estimators=15, max_depth=5), 'ciao', X_train, y_train, cv = ShuffleSplit(n_splits=20, test_size=0.2, random_state=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 3-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - An object to be used as a cross-validation generator.\n",
    "          - An iterable yielding train/test splits.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : integer, optional\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['playerShort', 'player', 'club', 'leagueCountry', 'birthday', 'height',\n",
       "       'weight', 'position', 'games', 'victories', 'ties', 'defeats', 'goals',\n",
       "       'yellowCards', 'yellowReds', 'redCards', 'photoID', 'rater1', 'rater2',\n",
       "       'refNum', 'refCountry', 'Alpha_3', 'meanIAT', 'nIAT', 'seIAT',\n",
       "       'meanExp', 'nExp', 'seExp'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_ref = data_clean.groupby('refNum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "agg_df = data_ref.agg({'meanIAT':'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meanIAT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>refNum</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.326391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.203375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.325185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.322177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.334684</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         meanIAT\n",
       "refNum          \n",
       "1       0.326391\n",
       "2       0.203375\n",
       "4       0.325185\n",
       "6       0.322177\n",
       "7       0.334684"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "referee_to_delete = agg_df[agg_df['meanIAT'].isnull()].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prova = data_clean[(data_clean['refNum'] != referee_to_delete[0]) & (data_clean['refNum'] != referee_to_delete[1])\n",
    "                  & (data_clean['refNum'] != referee_to_delete[2]) & (data_clean['refNum'] != referee_to_delete[3])\n",
    "                  & (data_clean['refNum'] != referee_to_delete[4]) & (data_clean['refNum'] != referee_to_delete[5])\n",
    "                  & (data_clean['refNum'] != referee_to_delete[6]) & (data_clean['refNum'] != referee_to_delete[7])\n",
    "                  & (data_clean['refNum'] != referee_to_delete[8])& (data_clean['refNum'] != referee_to_delete[9])\n",
    "                  & (data_clean['refNum'] != referee_to_delete[10])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster players with dark and light skin colors <a name=\"task2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
