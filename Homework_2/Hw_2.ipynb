{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 02 - Data from the Web\n",
    "\n",
    "*Remark:* whether you are interested in running all the cells (including the scraping) do that, otherwise, if you want just run the analysis, in the folder `Data_bachelor/` or `Data_master/` you find all the data used in the further analysis. For the tidiness of the notebook the functions used in the script are stored in the file `scraping_function.py`, `analysis_bachelor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import glob\n",
    "from scraping_function import *\n",
    "from analysis_bachelor import *\n",
    "import os\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1\n",
    "\n",
    "The final goal of the task is to verify whether the difference between the number of months that girls took from the first to the sixth semester and the number of months taken by boys, in average, is statistically significant.\n",
    "\n",
    "Before accomplishing the task we need to pass through the following steps:\n",
    "\n",
    "1. Obtain all the data for the Bachelor students, starting from 2007.\n",
    "2. Keep only the students for which you have an entry for both Bachelor semestre 1 and Bachelor semestre 6.\n",
    "3. Compute how many months it took each student to go from the first to the sixth semester.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we do, in order to retrieve the data of interest, is to parse the `html` source related to the `IS-Academia` web page (the one where you have to fill in the form to get the enrolled students). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Request the html source for the URL\n",
    "r = requests.get('http://isa.epfl.ch/imoniteur_ISAP/!GEDPUBLICREPORTS.filter?ww_i_reportModel=133685247')\n",
    "html = r.content\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of *Postman* we identify the *parameters* useful to get the data. They result to be the following:\n",
    "\n",
    "* `ww_x_GPS`\n",
    "* `ww_i_reportmodel`\n",
    "* `ww_i_reportModelXsl`\n",
    "* `ww_x_UNITE_ACAD`\n",
    "* `ww_x_PERIODE_ACAD`\n",
    "* `ww_x_PERIODE_PEDAGO`\n",
    "* `ww_x_HIVERETE`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gather them into a dictionary of the following form:\n",
    "\n",
    "                 {'parameter_1' : {'name_1' : value_1, ..., 'name_n' : value_n},\n",
    "                  'parameter_2' : {'name_1' : value_1, ..., 'name_n' : value_n},\n",
    "                   ...........,\n",
    "                  'parameter_k' : {'name_1' : value_1, ..., 'name_n' : value_n}}\n",
    " \n",
    "Where `parameter_X` is the name the relavant parameter to retrieve, `name_X` and `value_X` are two possible expressions of the parameter (string and numerical respectively). The function to do that is `create_parameter_dict` and it's stored in the [file](Applied data analysis - ADA/scaping_function.py) aforementioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the dictionary of parameters\n",
    "par = [\"ww_x_UNITE_ACAD\", \"ww_x_PERIODE_ACAD\", 'ww_x_PERIODE_PEDAGO', 'ww_x_HIVERETE']\n",
    "dic_par = create_parameter_dict(par, soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we choose and define the procedure to retrieve the information. After analyzing various `URLs`, with the help of *Postman*, we decide that we are going to make in total six requests, each of those consists of asking the list of the students for all the years related to a specific *pedagogic period*. The `URLs` have the following form:\n",
    "* `ww_x_GPS = -1` : in this way we gather all the data of the page\n",
    "* `ww_i_reportmodel = 133685247` : standard value of the parameter\n",
    "* `ww_i_reportModelXsl = dic_par['ww_i_reportModelXsl']['html']` : that specify the form of the data (`html`)\n",
    "* `ww_x_UNITE_ACAD = dic_par['ww_x_UNITE_ACAD']['Informatique']` : the value that corresponds to the students of Informatique\n",
    "* `ww_x_PERIODE_PEDAGO: dic_par['ww_x_PERIODE_PEDAGO'][semester]` : the code relative to a specific semester\n",
    "We don't specify anything about the season and the year beacause we decide to retrive everything related to a particular semester."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the list of the URLs we are making the request to\n",
    "urls = []\n",
    "\n",
    "# For each possible semester, \n",
    "for semester in dic_par['ww_x_PERIODE_PEDAGO']:\n",
    "    # Except for those known as semester 5b and 6b that DON'T contain data\n",
    "    if semester.startswith('Bachelor') and semester[-1] != 'b':  \n",
    "        # We define the link to append to our list\n",
    "        url_data = 'http://isa.epfl.ch/imoniteur_ISAP/!GEDPUBLICREPORTS.html?ww_x_GPS=-1&ww_i_reportModel=133685247&ww_i_reportModelXsl=' + dic_par['ww_i_reportModelXsl']['html'] + '&ww_x_UNITE_ACAD=' + dic_par['ww_x_UNITE_ACAD']['Informatique'] + '&ww_x_PERIODE_PEDAGO=' + dic_par['ww_x_PERIODE_PEDAGO'][semester]\n",
    "        urls.append(url_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the list of links is ready, we proceed retrieving data. To do that we are going to use a function `retrieve_data` which does the following:\n",
    "- The gathered `html` source code is transformed in a `DataFrame`. \n",
    "Since the `html` contains more than one table..\n",
    "- We are going to split the frame (using the function `fun_rec`) \n",
    "Below an image which tries to explain the procedure used to split the big `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"980\"\n",
       "            height=\"500\"\n",
       "            src=\"disegno_pdf.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0xfb4e8cbeb8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('disegno_pdf.pdf', width=980, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We clean the data using the `clean_df` function, in this step we keep the data of the years of interest \n",
    "- And finally save the cleaned `DFs` in `.csv` files (using the `save_data` function)\n",
    "- The file Data_bachelor is emptied and the csv files are loaded each time we execute the cell, this was done for modularity and flexibility reasons, in order for example to further select the year, or the pedagogic period.\n",
    "- An Important note here is that we do not take in account the ongoing academic year (2016-2017) for the simple reason that we cannot know if a student will really finish the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  request sent.\n",
      "Data collected from the  1  request.\n",
      "**************************************************\n",
      "2  request sent.\n",
      "Data collected from the  2  request.\n",
      "**************************************************\n",
      "3  request sent.\n",
      "Data collected from the  3  request.\n",
      "**************************************************\n",
      "4  request sent.\n",
      "Data collected from the  4  request.\n",
      "**************************************************\n",
      "5  request sent.\n",
      "Data collected from the  5  request.\n",
      "**************************************************\n",
      "6  request sent.\n",
      "Data collected from the  6  request.\n",
      "**************************************************\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "### TODO Pawel: wrap it in a function\n",
    "\n",
    "# Creating an empty directory if it doesn't exist\n",
    "try:\n",
    "    os.mkdir('Data_bachelor')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Changing the permissions in order to avoid problems\n",
    "os.chmod('Data_bachelor', 0o777)\n",
    "\n",
    "# Remove old files from this directory\n",
    "files = glob.glob('Data_bachelor/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "# Downloading the data into the 'Data_bachelor' directory\n",
    "k = 0\n",
    "for ur in urls:\n",
    "    k += 1\n",
    "    print ( k, ' request sent.')\n",
    "    save_data(ur, 'Data_bachelor/') # funtion defined in separate file\n",
    "    print ('Data collected from the ', k, ' request.')\n",
    "    print ('*****'*10)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2\n",
    "Now that we have all the data stored, we import and start to analyze them!\n",
    "\n",
    "We read the `.csv` fies as `DataFrame`, thus we concatenate them on the axis 0. It's done by the `import_data` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xe9 in position 7: unexpected end of data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-ab29f3de7aa1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimport_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Data_bachelor/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\josselin\\Desktop\\ADA\\ADA_Homeworks\\Homework_2\\analysis_bachelor.py\u001b[0m in \u001b[0;36mimport_data\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mfiles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'*.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m# Create the df concatenating the list of all frames\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unnamed: 0'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m# Replace the name of a column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\josselin\\Desktop\\ADA\\ADA_Homeworks\\Homework_2\\analysis_bachelor.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mfiles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'*.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m# Create the df concatenating the list of all frames\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unnamed: 0'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m# Replace the name of a column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\josselin\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    560\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 562\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\josselin\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 315\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\josselin\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    643\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 645\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\josselin\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    797\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    798\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 799\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    800\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    801\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\josselin\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1211\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1213\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas\\parser.c:5129)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._get_header (pandas\\parser.c:7634)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xe9 in position 7: unexpected end of data"
     ]
    }
   ],
   "source": [
    "data = import_data('Data_bachelor/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we proceed accomplishing the task of this step, in fact we filter the df according to the required criteria. In particular we `groupby` the frame according to the *Sciper number* that is unique for each enrolled student. Then we keep only the students (group) that have in the domain *Pedagogic period* at least one *Semester 1 & Semester 6*. A more technical procedure is explained inside the function `extract_students` found in the library provided with the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_students = extract_students(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_students.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3\n",
    "\n",
    "In this step we compute how many months each student spent to get from the 1st to the 6th semester. The assumptions we rely on are the following:\n",
    "- A student reach the *Semester 6* whether he doesn't have to sit again any other semester. We make this assumption because we think that the choice of the 6th semester is useful to compute the time an epfl student need to graduated at bachelor\n",
    "- Each semester lasts 6 months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting the duration of studies for each student, since we want to get their names, we check whether the name could be considered as an index. The way we do that is that the number of *names* should be equal to the number of *Scipers*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(df_students['Nom Prénom'].unique()) == len(df_students['No_Sciper'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we're going to extract the male features. Thus, we `select` the *Civilité* to be *Monsieur* (male) and then `groupby` *Nom Prénom* which is an index, the last operation we operate is an aggregate function `size()*6` in order to get the number of pedagogic period attached to a person and multiply it by 6 in order to get the correspondant time duration in months to get from the first to the sixth semester."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_male_aggregated = pd.DataFrame(df_students[df_students['Civilité'] == 'Monsieur'].groupby('Nom Prénom').size()*6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print some aggregated information to get a feeling of the data that may be useful later, and especially for choosing the right statistical test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_male_aggregated.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ploting the data so we can apprehend what would be the distribution and to get better understanding of the data such as uncovering the presence of outliers etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(df_male_aggregated.values, bins = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to suppress outliers and look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_male_aggregated[df_male_aggregated[0] <= 65].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform the exact same operations for female students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_female_aggregated = pd.DataFrame(df_students[df_students['Civilité'] == 'Madame'].groupby('Nom Prénom').size()*6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_female_aggregated.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(df_female_aggregated.values, bins = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our observations of the data, we choose a standard t-test  @TODO EXPLAIN MORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stats.ttest_ind(df_male_aggregated, df_female_aggregated, equal_var=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the master exercise, we will use code and pipelines from the Bachelor study as we used modular and reusable code. We use the same procedure to craft urls and select the data needed, namely data with *ww_x_PERIODE_PEDAGO* having *master* included or *projet*.\n",
    "*IMPORTANT* We made strong assumptions facing the imprecise nature of the data :\n",
    "- A Master's without *Spécialisation* or *Mineur* takes two semesters.\n",
    "- A Master's with *Spécialisation* or *Mineur* takes three semesters.\n",
    "- The duration of the *Master's thesis* is not included because it's not corresponding with the *Projet Prin* or *Projet Autu*, the data being too scarse compared to the number of registered in *Master Semester X*. The thesis is mandatory and can be done at EPFL but also in the industry, for this reason, we choose not to include it in our estimation for the *stay at EPFL* duration, because for the majority of the students, the thesis will be done outside EPFL walls.\n",
    "- The *Projet* semester is considered as an optional semester project done at EPFL, its duration will be simply added to the time student spent at EPFL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the list of the URLs we are making the request to\n",
    "urls = []\n",
    "\n",
    "# For each possible semester, \n",
    "for semester in dic_par['ww_x_PERIODE_PEDAGO']:\n",
    "    # Except for those known as semester 5b and 6b that DON'T contain data\n",
    "    if semester.startswith('Master') or semester.startswith('Projet'):  \n",
    "        # We define the link to append to our list\n",
    "        url_data = 'http://isa.epfl.ch/imoniteur_ISAP/!GEDPUBLICREPORTS.html?ww_x_GPS=-1&ww_i_reportModel=133685247&ww_i_reportModelXsl=' + dic_par['ww_i_reportModelXsl']['html'] + '&ww_x_UNITE_ACAD=' + dic_par['ww_x_UNITE_ACAD']['Informatique'] + '&ww_x_PERIODE_PEDAGO=' + dic_par['ww_x_PERIODE_PEDAGO'][semester]\n",
    "        urls.append(url_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once urls are created we will load and store the data into *Data_master* folder, the procedure is exactly identical than the one used in the previous exercise, we again use `save_data` defined as a helper in a separate file consultable on the GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating an empty directory if it doesn't exist\n",
    "try:\n",
    "    os.mkdir('Data_master')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Changing the permissions in order to avoid problems\n",
    "os.chmod('Data_master', 0o777)\n",
    "\n",
    "# Remove old files from this directory\n",
    "files = glob.glob('Data_master/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "# Downloading the data into the 'Data_master' directory\n",
    "k = 0\n",
    "for ur in urls:\n",
    "    k += 1\n",
    "    print ( k, ' request sent.')\n",
    "    save_data(ur, 'Data_master/') # funtion defined in separate file\n",
    "    print ('Data collected from the ', k, ' request.')\n",
    "    print ('*****'*10)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data previously downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = import_data('Data_master/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verify that the data we retrieved is the one we were targeting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['Pedagogic period'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are interested by the time it took students to get their first 60 credits, as it is mandatory for everyone. In this first step we pre-select the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data to compute the time for 60 credits\n",
    "data_time = data[(data['Pedagogic period'] == ' Master semestre 1 \\xa0') | (data['Pedagogic period'] == ' Master semestre 2 \\xa0')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second step : we prepare a filter to isolate the data needed. We first `groupby` *No_Sciper* and to get an aggregate on the *Pedagogic Period*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select students that starts and ends master's first 60 credits at EPFL ---> master 1 and master 2\n",
    "# Group by the identification number of the students\n",
    "grouped_data = data_time.groupby('No_Sciper')\n",
    "    \n",
    "    \n",
    "# Define the semesters we want the students have been enrolled in.\n",
    "master_sem = pd.Series([' Master semestre 1 \\xa0', ' Master semestre 2 \\xa0'])\n",
    "    \n",
    "# Get the list of the students (we are interested in) No_Sciper\n",
    "students = []\n",
    "    \n",
    "# For each group\n",
    "for g in grouped_data.groups:\n",
    "        # We extract the Pedagocic periods \n",
    "    period_attended = grouped_data.get_group(g)['Pedagogic period']\n",
    "        # Whether both Semester 1 and semester 2 are included we select the student\n",
    "    if sum(master_sem.isin(period_attended)) == 2:\n",
    "        students.append(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the selection prepared previously to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pr = data_time.query('No_Sciper in @students')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that our processing is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pr['Pedagogic period'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply aggregate to get the time spent in months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# time from ma1 to ma2\n",
    "t = pr.groupby('No_Sciper').size()*6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Average time spent at epfl for mter students. just counting semester 1 and 2\n",
    "t.mean()\n",
    "t.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will additively compute the data relative to Minor and Specialization students, as they had to do their first two Master Semester. For such students we will just add the time of their Minor/Spec which corresponds to Master Semester 3. We look at the students having a semester 3 and that already did their first two semesters (we won't count the students having a specialization having only done semester 1 and 2 and not registered in semester 3 as they may have failed or have to redo one semester)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Minor/ spec\n",
    "df_minor = data[data['Pedagogic period'] == ' Master semestre 3 \\xa0']\n",
    "\n",
    "\n",
    "# In minor keep those that already do two semesters\n",
    "minor = df_minor.query('No_Sciper in @students')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply aggregate to get the time spent in months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# time to do minor or specialization\n",
    "m = minor.groupby('No_Sciper').size()*6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the optional project duration according to the assumption mentioned at the begining of the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_project = data[(data['Pedagogic period'] == ' Projet Master autom') |\n",
    "                  (data['Pedagogic period'] == ' Projet Master print')]\n",
    "project = df_project.query('No_Sciper in @students')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply aggregate to get the time spent in months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = project.groupby('No_Sciper').size()*6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Add these size to the t vector\n",
    "tot = (t.add(p, fill_value = 0)).add(m, fill_value = 0)\n",
    "tot.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_special = minor[minor['Spécialisation'].notnull()]\n",
    "special = df_special.query('No_Sciper in @students')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Average stay\n",
    "special.groupby('No_Sciper').agg({'Spécialisation' : 'first', 'Pedagogic period' : 'size'}).groupby('Spécialisation').mean()*6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_male_aggregated = pd.DataFrame(pr[pr['Civilité'] == 'Monsieur'].groupby('No_Sciper').size()*6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_male_aggregated.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_female_aggregated = pd.DataFrame(pr[pr['Civilité'] == 'Madame'].groupby('No_Sciper').size()*6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_female_aggregated.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tot_sorted = tot.sort_index(sort_remaining=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We filter for first semester of master studies in order to find the starting year of each student.\n",
    "# As it may happen that students have repeated the semesters we want to drop the other entries.\n",
    "# Since the data is sorted by date we drop following occurences of the master semester 1\n",
    "prova = pr[pr['Pedagogic period'] == ' Master semestre 1 \\xa0'].drop_duplicates(subset = ['No_Sciper']).sort_values(by = 'No_Sciper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prova.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prova['Length_study'] = tot_sorted.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prova_grouped = prova.groupby(['Civilité', 'Academic year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Monsieur - Try with agg fun and apply\n",
    "avg_men = {}\n",
    "avg_women = {}\n",
    "for g in prova_grouped.groups:\n",
    "    if g[0] == 'Madame':\n",
    "        avg_women[g[1]] = prova_grouped.get_group(g)['Length_study'].mean()\n",
    "    else:\n",
    "        avg_men[g[1]] = prova_grouped.get_group(g)['Length_study'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg_men"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg_women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "od = collections.OrderedDict(sorted(avg_men.items()))\n",
    "ood = collections.OrderedDict(sorted(avg_women.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add title and axis lables. Y axis between 12 and 24 for better visualisation of data\n",
    "# Since we have already dropped the data for 2016/2017 in the beginning,\n",
    "# we cannot consider the results for mean in the academic year 2015/2016 because the data related to the second year\n",
    "# of their studies aren't present in the data set. So we drop them in the charts.\n",
    "plt.plot(list(od.values())[:-1], '-ro', list(ood.values())[:-1], '-bo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_results = {}\n",
    "for year in sorted(prova['Academic year'].unique())[:-1]:\n",
    "    df_female = prova[(prova['Civilité'] == 'Madame') & (prova['Academic year'] == year) ]['Length_study']\n",
    "    df_male = prova[(prova['Civilité'] == 'Monsieur') & (prova['Academic year'] == year) ]['Length_study']\n",
    "    \n",
    "    t_test = stats.ttest_ind(df_male, df_female, equal_var=False)\n",
    "    test_results[year] = {'t-statistic' : t_test[0], 'p-value' : t_test[1]}\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
